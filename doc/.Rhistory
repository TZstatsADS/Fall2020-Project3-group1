ggplot(res_cv_xgboost_cv_results, aes(as.factor(nrounds), as.factor(eta), fill = mean_AUC)) +
geom_tile()
# Mean Error
# nrounds = 600
ggplot(res_cv_xgboost_cv_results[res_cv_xgboost_cv_results$nrounds == 600 & res_cv_xgboost_cv_results$gamma == 0 & res_cv_xgboost_cv_results$lambda == 0.05, ],
aes(x = as.factor(eta), y = mean_error,
ymin = mean_error - sd_error, ymax = mean_error + sd_error)) +
geom_crossbar() + theme(axis.text.x = element_text(angle = 90, hjust = 1))
# nrounds = 1200
ggplot(res_cv_xgboost_cv_results[res_cv_xgboost_cv_results$nrounds == 1200 & res_cv_xgboost_cv_results$gamma == 0 & res_cv_xgboost_cv_results$lambda == 0.05, ],
aes(x = as.factor(eta), y = mean_error,
ymin = mean_error - sd_error, ymax = mean_error + sd_error)) +
geom_crossbar() + theme(axis.text.x = element_text(angle = 90, hjust = 1))
# nrounds = 1800
ggplot(res_cv_xgboost_cv_results[res_cv_xgboost_cv_results$nrounds == 1800 & res_cv_xgboost_cv_results$gamma == 0 & res_cv_xgboost_cv_results$lambda == 0.05, ],
aes(x = as.factor(eta), y = mean_error,
ymin = mean_error - sd_error, ymax = mean_error + sd_error)) +
geom_crossbar() + theme(axis.text.x = element_text(angle = 90, hjust = 1))
# Mean AUC
# nrounds = 600
ggplot(res_cv_xgboost_cv_results[res_cv_xgboost_cv_results$nrounds == 600 & res_cv_xgboost_cv_results$gamma == 0 & res_cv_xgboost_cv_results$lambda == 0.05, ],
aes(x = as.factor(eta), y = mean_AUC,
ymin = mean_AUC - sd_AUC, ymax = mean_AUC + sd_AUC)) +
geom_crossbar() + theme(axis.text.x = element_text(angle = 90, hjust = 1))
# nrounds = 1200
ggplot(res_cv_xgboost_cv_results[res_cv_xgboost_cv_results$nrounds == 1200 & res_cv_xgboost_cv_results$gamma == 0 & res_cv_xgboost_cv_results$lambda == 0.05, ],
aes(x = as.factor(eta), y = mean_AUC,
ymin = mean_AUC - sd_AUC, ymax = mean_AUC + sd_AUC)) +
geom_crossbar() + theme(axis.text.x = element_text(angle = 90, hjust = 1))
# nrounds = 1800
ggplot(res_cv_xgboost_cv_results[res_cv_xgboost_cv_results$nrounds == 1800 & res_cv_xgboost_cv_results$gamma == 0 & res_cv_xgboost_cv_results$lambda == 0.05, ],
aes(x = as.factor(eta), y = mean_AUC,
ymin = mean_AUC - sd_AUC, ymax = mean_AUC + sd_AUC)) +
geom_crossbar() + theme(axis.text.x = element_text(angle = 90, hjust = 1))
# Chunk 21: best_model_xgboost
par_best_res_cv_xgboost_cv_results_ind <-
which(res_cv_xgboost_cv_results$mean_AUC == max(res_cv_xgboost_cv_results$mean_AUC))
par_best_res_cv_xgboost_cv_results_eta <- res_cv_xgboost_cv_results$eta[par_best_res_cv_xgboost_cv_results_ind]
par_best_res_cv_xgboost_cv_results_lambda <- res_cv_xgboost_cv_results$lambda[par_best_res_cv_xgboost_cv_results_ind]
par_best_res_cv_xgboost_cv_results_gamma <- res_cv_xgboost_cv_results$gamma[par_best_res_cv_xgboost_cv_results_ind]
par_best_res_cv_xgboost_cv_results_nrounds <- res_cv_xgboost_cv_results$nrounds[par_best_res_cv_xgboost_cv_results_ind]
# Chunk 22: final_train_xgboost
if (run.train.xgboost) {
# training weights
weight_train <- rep(NA, length(label_train))
for (v in unique(label_train)){
weight_train[label_train == v] = 0.5 * length(label_train) / length(label_train[label_train == v])
}
if (sample.reweight){
tm_train_xgboost <- system.time(fit_train_xgboost <- train(features = feature_train, labels = label_train,
w =  weight_train,
eta_val = par_best_res_cv_xgboost_cv_results_eta,
lmd = par_best_res_cv_xgboost_cv_results_lambda,
gam = par_best_res_cv_xgboost_cv_results_gamma,
nr = par_best_res_cv_xgboost_cv_results_nrounds))
} else {
tm_train_xgboost <- system.time(fit_train_xgboost <- train(features = feature_train, labels = label_train,
w =  NULL,
eta_val = par_best_res_cv_xgboost_cv_results_eta,
lmd = par_best_res_cv_xgboost_cv_results_lambda,
gam = par_best_res_cv_xgboost_cv_results_gamma,
nr = par_best_res_cv_xgboost_cv_results_nrounds))
}
save(fit_train_xgboost, tm_train_xgboost, file="../output/fit_train_xgboost.RData")
} else {
load(file="../output/fit_train_xgboost.RData")
}
# Chunk 23
feature_test <- as.matrix(dat_test[, -6007])
label_test <- as.integer(dat_test$label)
tm_test_xgboost= NA
if(run.test.xgboost){
load(file="../output/fit_train_xgboost.RData")
tm_test_xgboost <- system.time({prob_pred <- predict(fit_train_xgboost, feature_test);
label_pred <- ifelse(prob_pred >= 0.5, 1, 0)})
}
# Chunk 24
## reweight the test data to represent a balanced label distribution
weight_test <- rep(NA, length(label_test))
for (v in unique(label_test)){
weight_test[label_test == v] = 0.5 * length(label_test) / length(label_test[label_test == v])
}
# convert the original 1-2 class into numeric 0s and 1s
label_test <- ifelse(label_test == 2, 0, 1)
accu <- sum(weight_test * (label_pred == label_test)) / sum(weight_test)
tpr.fpr <- WeightedROC(prob_pred, label_test, weight_test)
auc <- WeightedAUC(tpr.fpr)
# Chunk 25
cat("The accuracy of the xgboost model (", "eta = ", par_best_res_cv_xgboost_cv_results_eta,
", nrounds = ", par_best_res_cv_xgboost_cv_results_nrounds, ", lambda = ", par_best_res_cv_xgboost_cv_results_lambda,
", gamma = ", par_best_res_cv_xgboost_cv_results_gamma,
") is ", accu*100, "%.\n", sep = "")
cat("The AUC of the xgboost model (", "eta = ", par_best_res_cv_xgboost_cv_results_eta,
", nrounds = ", par_best_res_cv_xgboost_cv_results_nrounds, ", lambda = ", par_best_res_cv_xgboost_cv_results_lambda,
", gamma = ", par_best_res_cv_xgboost_cv_results_gamma,
") is ", auc, ".\n", sep = "")
# Chunk 26: running_time_xgboost
cat("Time for training xgboost model = ", tm_train_xgboost[1], " seconds \n", sep = "")
cat("Time for testing xgboost model = ", tm_test_xgboost[1], " seconds \n" , sep = "")
train_dir <- "~/train_set/"
run.train.gbm
# Chunk 1
if(!require("EBImage")){
install.packages("BiocManager")
BiocManager::install("EBImage")
}
if(!require("R.matlab")){
install.packages("R.matlab")
}
if(!require("readxl")){
install.packages("readxl")
}
if(!require("dplyr")){
install.packages("dplyr")
}
if(!require("readxl")){
install.packages("readxl")
}
if(!require("ggplot2")){
install.packages("ggplot2")
}
if(!require("caret")){
install.packages("caret")
}
if(!require("glmnet")){
install.packages("glmnet")
}
if(!require("WeightedROC")){
install.packages("WeightedROC")
}
if(!require("gbm")){
install.packages("gbm")
}
if(!require("xgboost")){
install.packages("xgboost")
}
if(!require("randomForest")){
install.packages("randomForest")
}
library(R.matlab)
library(readxl)
library(dplyr)
library(EBImage)
library(ggplot2)
library(caret)
library(glmnet)
library(WeightedROC)
library(gbm)
library(xgboost)
library(randomForest)
# install other packages as needed using the format above
# Chunk 3
# change the directory of the data to where it's stored in your local drive as needed
# train_dir <- "../data/train_set/" # This will be modified for different data sets.
train_dir <- "~/train_set/"
train_image_dir <- paste(train_dir, "images/", sep="")
train_pt_dir <- paste(train_dir,  "points/", sep="")
train_label_path <- paste(train_dir, "label.csv", sep="")
# Chunk 4: exp_setup
K <- 5  # number of CV folds
run.fudicial.list <- FALSE
run.feature.train <- FALSE # process features for training set
run.feature.test <- FALSE # process features for test set
sample.reweight <- TRUE # run sample reweighting in model training
run.cv.gbm <- FALSE # run cross-validation on the training set for gbm
run.train.gbm <- FALSE # run evaluation on entire train set
run.test.gbm <- TRUE # run evaluation on an independent test set
run.cv.xgboost <- FALSE # run cross-validation on the training set for xgboost
run.train.xgboost <- FALSE # run evaluation on entire train set
run.test.xgboost <- TRUE # run evaluation on an independent test set
# add controls here to make if else statements to either cross-validate, test, train, or to just load saved data
# for xgboost, we need to also train and test each time we knit to record the time for the model
# Chunk 5: model_setup
# hyperparameters for our models
# gbm model (baseline)
hyper_grid_gbm <- expand.grid(
shrinkage = c(0.001, 0.005, 0.010, 0.050, 0.100),
n.trees = c(600, 1200, 1800)
)
# xgboost model
hyper_grid_xgboost <- expand.grid(
eta = c(0.01, 0.05, 0.1, 0.2, 0.3),
lambda = c(0.001, 0.005, 0.010, 0.050, 0.100),
gamma = c(0, 5),
nrounds = c(600, 1200, 1800)
)
# random forest model
# add more hyperparameters for each model as needed
# Chunk 6
#train-test split
info <- read.csv(train_label_path)
n <- nrow(info)
n_train <- round(n*(4/5), 0)
train_idx <- sample(info$Index, n_train, replace = F)
test_idx <- setdiff(info$Index, train_idx)
# Chunk 7: read fiducial points
n_files <- length(list.files(train_image_dir))
if (run.fudicial.list){
#function to read fiducial points
#input: index
#output: matrix of fiducial points corresponding to the index
readMat.matrix <- function(index){
return(round(readMat(paste0(train_pt_dir, sprintf("%04d", index), ".mat"))[[1]],0))
}
#load fiducial points
fiducial_pt_list <- lapply(1:n_files, readMat.matrix)
save(fiducial_pt_list, file="../output/fiducial_pt_list.RData")
} else {
load(file="../output/fiducial_pt_list.RData")
}
# Chunk 8: feature
source("../lib/feature.R")
tm_feature_train <- NA
if(run.feature.train){
tm_feature_train <- system.time(dat_train <- feature(fiducial_pt_list, train_idx))
save(dat_train, tm_feature_train, file="../output/feature_train.RData")
}else{
load(file="../output/feature_train.RData")
}
tm_feature_test <- NA
if(run.feature.test){
tm_feature_test <- system.time(dat_test <- feature(fiducial_pt_list, test_idx))
save(dat_test, tm_feature_test, file="../output/feature_test.RData")
}else{
load(file="../output/feature_test.RData")
}
# Chunk 9: loadlib_gbm
source("../lib/train_gbm.R")
source("../lib/test_gbm.R")
source("../lib/cross_validation_gbm.R")
# Chunk 10: runcv_gbm
feature_train = as.matrix(dat_train[, -6007])
label_train = as.integer(dat_train$label)
if(run.cv.gbm){
res_cv <- matrix(0, nrow = nrow(hyper_grid_gbm), ncol = 4)
for(i in 1:nrow(hyper_grid_gbm)){
cat("n.trees = ", hyper_grid_gbm$n.trees[i], ",
shrinkage = ", hyper_grid_gbm$shrinkage[i],"\n", sep = "")
res_cv[i,] <- cv.function(features = feature_train, labels = label_train,
num_trees = hyper_grid_gbm$n.trees[i],
shrink = hyper_grid_gbm$shrinkage[i],
K, reweight = sample.reweight)
save(res_cv, file="../output/res_cv_gbm.RData")
}
}else{
load("../output/res_cv_gbm.RData")
}
res_cv_gbm
res_cv
res_cv_gbm <- as.data.frame(res_cv)
colnames(res_cv_gbm) <- c("mean_error", "sd_error", "mean_AUC", "sd_AUC")
gbm_cv_results = data.frame(hyper_grid_gbm, res_cv_gbm)
# a subset of cross-validated gbm models (15 total)
# see appendix for full table
gbm_cv_results[gbm_cv_results$n.trees == 1200, ]
?sort
sort(gbm_cv_results, decreasing = TRUE, by = mean_AUC)
sort(gbm_cv_results, decreasing = TRUE, mean_AUC)
sort(gbm_cv_results, decreasing = TRUE, method = mean_AUC)
sort(gbm_cv_results, decreasing = TRUE)
# a subset of cross-validated gbm models (15 total)
# see appendix for full table
gbm_cv_results[order(gbm_cv_results$mean_AUC), ]
?order
# a subset of cross-validated gbm models (15 total)
# see appendix for full table
gbm_cv_results[order(gbm_cv_results$mean_AUC, decreasing = TRUE), ]
# a subset of cross-validated gbm models (15 total)
# see appendix for full table
gbm_cv_results[order(gbm_cv_results$mean_AUC, decreasing = TRUE), ][1:5, ]
par_best_gbm_ind <- gbm_cv_results[order(gbm_cv_results$mean_AUC, decreasing = TRUE), ][2, ]
par_best_gbm_ind
par_best_gbm_ind <- gbm_cv_results[order(gbm_cv_results$mean_AUC, decreasing = TRUE), ][2, ]
par_best_gbm_shrinkage <- gbm_cv_results$shrinkage[par_best_gbm_ind]
par_best_gbm_n.trees <- gbm_cv_results$n.trees[par_best_gbm_ind]
run.train.gbm
if (run.train.gbm) {
# training weights
weight_train <- rep(NA, length(label_train))
for (v in unique(label_train)){
weight_train[label_train == v] = 0.5 * length(label_train) / length(label_train[label_train == v])
}
if (sample.reweight){
tm_train_gbm <- system.time(fit_train_gbm <- train(feature_train, label_train, w = weight_train,
num_trees = par_best_gbm_n.trees,
shrink = par_best_gbm_shrinkage))
} else {
tm_train_gbm <- system.time(fit_train_gbm <- train(feature_train, label_train, w = NULL,
num_trees = par_best_gbm_n.trees,
shrink = par_best_gbm_shrinkage))
}
save(fit_train_gbm, tm_train_gbm, file="../output/fit_train_gbm.RData")
} else {
load(file="../output/fit_train_gbm.RData")
}
feature_test <- as.matrix(dat_test[, -6007])
label_test <- as.integer(dat_test$label)
tm_test_gbm = NA
if(run.test.gbm){
load(file="../output/fit_train_gbm.RData")
tm_test_gbm <- system.time({prob_pred <- test(fit_train_gbm, feature_test, pred.type = 'response');
label_pred <- ifelse(prob_pred >= 0.5, 1, 0)})
}
weight_test <- rep(NA, length(label_test))
for (v in unique(label_test)){
weight_test[label_test == v] = 0.5 * length(label_test) / length(label_test[label_test == v])
}
# convert the original 1-2 class into numeric 0s and 1s
label_test <- ifelse(label_test == 2, 0, 1)
accu <- sum(weight_test * (label_pred == label_test)) / sum(weight_test)
tpr.fpr <- WeightedROC(prob_pred, label_test, weight_test)
auc <- WeightedAUC(tpr.fpr)
cat("The accuracy of the gbm model (", "shinkage = ", par_best_gbm_shrinkage, ", n.trees = ", par_best_gbm_n.trees, ") is ", accu*100, "%.\n", sep = "")
cat("The AUC of the gbm model (", "shinkage = ", par_best_gbm_shrinkage, ", n.trees = ", par_best_gbm_n.trees, ") is ", auc, ".\n", sep = "")
par_best_gbm_shrinkage <- gbm_cv_results$shrinkage[par_best_gbm_ind]
par_best_gbm_n.trees <- gbm_cv_results$n.trees[par_best_gbm_ind]
if (run.train.gbm) {
# training weights
weight_train <- rep(NA, length(label_train))
for (v in unique(label_train)){
weight_train[label_train == v] = 0.5 * length(label_train) / length(label_train[label_train == v])
}
if (sample.reweight){
tm_train_gbm <- system.time(fit_train_gbm <- train(feature_train, label_train, w = weight_train,
num_trees = par_best_gbm_n.trees,
shrink = par_best_gbm_shrinkage))
} else {
tm_train_gbm <- system.time(fit_train_gbm <- train(feature_train, label_train, w = NULL,
num_trees = par_best_gbm_n.trees,
shrink = par_best_gbm_shrinkage))
}
save(fit_train_gbm, tm_train_gbm, file="../output/fit_train_gbm.RData")
} else {
load(file="../output/fit_train_gbm.RData")
}
cat("The accuracy of the gbm model (", "shinkage = ", par_best_gbm_shrinkage, ", n.trees = ", par_best_gbm_n.trees, ") is ", accu*100, "%.\n", sep = "")
cat("The AUC of the gbm model (", "shinkage = ", par_best_gbm_shrinkage, ", n.trees = ", par_best_gbm_n.trees, ") is ", auc, ".\n", sep = "")
par_best_gbm_ind <- gbm_cv_results[order(gbm_cv_results$mean_AUC, decreasing = TRUE), ][2, ]
par_best_gbm_shrinkage <- gbm_cv_results$shrinkage[par_best_gbm_ind]
par_best_gbm_n.trees <- gbm_cv_results$n.trees[par_best_gbm_ind]
par_best_gbm_ind
par_best_gbm_ind <- 4
gbm_cv_results[order(gbm_cv_results$mean_AUC, decreasing = TRUE), ][2, ]
par_best_gbm_ind <- 4
par_best_gbm_shrinkage <- gbm_cv_results$shrinkage[par_best_gbm_ind]
par_best_gbm_n.trees <- gbm_cv_results$n.trees[par_best_gbm_ind]
cat("The accuracy of the gbm model (", "shinkage = ", par_best_gbm_shrinkage, ", n.trees = ", par_best_gbm_n.trees, ") is ", accu*100, "%.\n", sep = "")
cat("The AUC of the gbm model (", "shinkage = ", par_best_gbm_shrinkage, ", n.trees = ", par_best_gbm_n.trees, ") is ", auc, ".\n", sep = "")
cat("Time for constructing training features = ", tm_feature_train[1], " seconds \n", sep = "")
cat("Time for constructing testing features = ", tm_feature_test[1], " seconds \n", sep = "")
cat("Time for training gbm model = ", tm_train_gbm[1], " seconds \n", sep = "")
cat("Time for testing gbm model = ", tm_test_gbm[1], " seconds \n" , sep = "")
source("../lib/train_xgboost.R")
source("../lib/test_xgboost.R")
source("../lib/cross_validation_xgboost.R")
source("../lib/train_xgboost.R")
source("../lib/test_xgboost.R")
source("../lib/cross_validation_xgboost.R")
feature_train = as.matrix(dat_train[, -6007])
feature_train = as.matrix(dat_train[, -6007])
label_train = as.integer(dat_train$label)
if(run.cv.xgboost){
res_cv <- matrix(0, nrow = nrow(hyper_grid_xgboost), ncol = 4)
for (i in 1:nrow(hyper_grid_xgboost)){
print(i)
res_cv[i,] <- cv.function(features = feature_train, labels = label_train,
K,
eta_val = hyper_grid_xgboost$eta[i],
lmd = hyper_grid_xgboost$lambda[i],
gam = hyper_grid_xgboost$gamma[i],
nr = hyper_grid_xgboost$nrounds[i])
save(res_cv, file="../output/res_cv_xgboost.RData")
}
}else{
load("../output/res_cv_xgboost.RData")
}
res_cv_xgboost <- as.data.frame(res_cv)
colnames(res_cv_xgboost) <- c("mean_error", "sd_error", "mean_AUC", "sd_AUC")
res_cv_xgboost_cv_results = data.frame(hyper_grid_xgboost, res_cv_xgboost)
res_cv_xgboost_cv_results
# a subset of cross-validated xgboost models ordered by mean AUC (150 total)
# see appendix for full table
res_cv_xgboost_cv_results[order(res_cv_xgboost_cv_results$mean_AUC), ][1:5, ]
feature_train = as.matrix(dat_train[, -6007])
label_train = as.integer(dat_train$label)
if(run.cv.xgboost){
res_cv <- matrix(0, nrow = nrow(hyper_grid_xgboost), ncol = 4)
for (i in 1:nrow(hyper_grid_xgboost)){
print(i)
res_cv[i,] <- cv.function(features = feature_train, labels = label_train,
K,
eta_val = hyper_grid_xgboost$eta[i],
lmd = hyper_grid_xgboost$lambda[i],
gam = hyper_grid_xgboost$gamma[i],
nr = hyper_grid_xgboost$nrounds[i])
save(res_cv, file="../output/res_cv_xgboost.RData")
}
}else{
load("../output/res_cv_xgboost.RData")
}
res_cv_xgboost <- as.data.frame(res_cv)
colnames(res_cv_xgboost) <- c("mean_error", "sd_error", "mean_AUC", "sd_AUC")
res_cv_xgboost_cv_results = data.frame(hyper_grid_xgboost, res_cv_xgboost)
# a subset of cross-validated gbm models ordered by mean AUC (15 total)
# see appendix for full table
gbm_cv_results[order(gbm_cv_results$mean_AUC, decreasing = TRUE), ][1:5, ]
# a subset of cross-validated xgboost models ordered by mean AUC (150 total)
# see appendix for full table
res_cv_xgboost_cv_results[order(res_cv_xgboost_cv_results$mean_AUC), ][1:5, ]
if(run.cv.xgboost){
res_cv <- matrix(0, nrow = nrow(hyper_grid_xgboost), ncol = 4)
for (i in 1:nrow(hyper_grid_xgboost)){
print(i)
res_cv[i,] <- cv.function(features = feature_train, labels = label_train,
K,
eta_val = hyper_grid_xgboost$eta[i],
lmd = hyper_grid_xgboost$lambda[i],
gam = hyper_grid_xgboost$gamma[i],
nr = hyper_grid_xgboost$nrounds[i])
save(res_cv, file="../output/res_cv_xgboost.RData")
}
}else{
load("../output/res_cv_xgboost.RData")
}
res_cv_xgboost <- as.data.frame(res_cv)
res_cv_xgboost
res_cv_xgboost <- as.data.frame(res_cv)
colnames(res_cv_xgboost) <- c("mean_error", "sd_error", "mean_AUC", "sd_AUC")
res_cv_xgboost_cv_results = data.frame(hyper_grid_xgboost, res_cv_xgboost)
res_cv_xgboost_cv_results
# a subset of cross-validated xgboost models ordered by mean AUC (150 total)
# see appendix for full table
res_cv_xgboost_cv_results[order(res_cv_xgboost_cv_results$mean_AUC, decreasing = TRUE), ][1:5, ]
res_cv_xgboost_cv_results[order(res_cv_xgboost_cv_results$mean_AUC, decreasing = TRUE), ][1:5, ]
# Mean Error
ggplot(res_cv_xgboost_cv_results[res_cv_xgboost_cv_results$gamma == 0 & res_cv_xgboost_cv_results$lambda == 0.05], aes(as.factor(nrounds), as.factor(eta), fill = mean_error)) +
geom_tile()
# Mean Error
ggplot(res_cv_xgboost_cv_results[res_cv_xgboost_cv_results$gamma == 0 & res_cv_xgboost_cv_results$lambda == 0.05, ], aes(as.factor(nrounds), as.factor(eta), fill = mean_error)) +
geom_tile()
ggplot(res_cv_xgboost_cv_results[res_cv_xgboost_cv_results$gamma == 0 & res_cv_xgboost_cv_results$lambda == 0.05, ], aes(as.factor(nrounds), as.factor(eta), fill = mean_AUC)) +
geom_tile()
ggplot(res_cv_xgboost_cv_results[res_cv_xgboost_cv_results$gamma == 0 & res_cv_xgboost_cv_results$lambda == 0.10, ], aes(as.factor(nrounds), as.factor(eta), fill = mean_AUC)) +
geom_tile()
res_cv_xgboost_cv_results[order(res_cv_xgboost_cv_results$mean_AUC, decreasing = TRUE), ][2, ]
# a subset of cross-validated xgboost models ordered by mean AUC (150 total)
# see appendix for full table
res_cv_xgboost_cv_results[order(res_cv_xgboost_cv_results$mean_AUC, decreasing = TRUE), ][1:5, ]
res_cv_xgboost_cv_results[order(res_cv_xgboost_cv_results$mean_AUC, decreasing = TRUE), ][2, ]
res_cv_xgboost_cv_results[order(res_cv_xgboost_cv_results$mean_AUC, decreasing = TRUE), ][2, ]
par_best_res_cv_xgboost_cv_results_ind <- 23
par_best_res_cv_xgboost_cv_results_eta <-
res_cv_xgboost_cv_results$eta[par_best_res_cv_xgboost_cv_results_ind]
par_best_res_cv_xgboost_cv_results_lambda <-
res_cv_xgboost_cv_results$lambda[par_best_res_cv_xgboost_cv_results_ind]
par_best_res_cv_xgboost_cv_results_gamma <-
res_cv_xgboost_cv_results$gamma[par_best_res_cv_xgboost_cv_results_ind]
par_best_res_cv_xgboost_cv_results_nrounds <-
res_cv_xgboost_cv_results$nrounds[par_best_res_cv_xgboost_cv_results_ind]
if (run.train.xgboost) {
# training weights
weight_train <- rep(NA, length(label_train))
for (v in unique(label_train)){
weight_train[label_train == v] = 0.5 * length(label_train) / length(label_train[label_train == v])
}
if (sample.reweight){
tm_train_xgboost <- system.time(fit_train_xgboost <- train(features = feature_train, labels = label_train,
w =  weight_train,
eta_val = par_best_res_cv_xgboost_cv_results_eta,
lmd = par_best_res_cv_xgboost_cv_results_lambda,
gam = par_best_res_cv_xgboost_cv_results_gamma,
nr = par_best_res_cv_xgboost_cv_results_nrounds))
} else {
tm_train_xgboost <- system.time(fit_train_xgboost <- train(features = feature_train, labels = label_train,
w =  NULL,
eta_val = par_best_res_cv_xgboost_cv_results_eta,
lmd = par_best_res_cv_xgboost_cv_results_lambda,
gam = par_best_res_cv_xgboost_cv_results_gamma,
nr = par_best_res_cv_xgboost_cv_results_nrounds))
}
save(fit_train_xgboost, tm_train_xgboost, file="../output/fit_train_xgboost.RData")
} else {
load(file="../output/fit_train_xgboost.RData")
}
feature_test <- as.matrix(dat_test[, -6007])
label_test <- as.integer(dat_test$label)
tm_test_xgboost= NA
if(run.test.xgboost){
load(file="../output/fit_train_xgboost.RData")
tm_test_xgboost <- system.time({prob_pred <- predict(fit_train_xgboost, feature_test);
label_pred <- ifelse(prob_pred >= 0.5, 1, 0)})
}
## reweight the test data to represent a balanced label distribution
weight_test <- rep(NA, length(label_test))
for (v in unique(label_test)){
weight_test[label_test == v] = 0.5 * length(label_test) / length(label_test[label_test == v])
}
# convert the original 1-2 class into numeric 0s and 1s
label_test <- ifelse(label_test == 2, 0, 1)
accu <- sum(weight_test * (label_pred == label_test)) / sum(weight_test)
tpr.fpr <- WeightedROC(prob_pred, label_test, weight_test)
auc <- WeightedAUC(tpr.fpr)
cat("The accuracy of the xgboost model (", "eta = ", par_best_res_cv_xgboost_cv_results_eta,
", nrounds = ", par_best_res_cv_xgboost_cv_results_nrounds, ", lambda = ", par_best_res_cv_xgboost_cv_results_lambda,
", gamma = ", par_best_res_cv_xgboost_cv_results_gamma,
") is ", accu*100, "%.\n", sep = "")
cat("The AUC of the xgboost model (", "eta = ", par_best_res_cv_xgboost_cv_results_eta,
", nrounds = ", par_best_res_cv_xgboost_cv_results_nrounds, ", lambda = ", par_best_res_cv_xgboost_cv_results_lambda,
", gamma = ", par_best_res_cv_xgboost_cv_results_gamma,
") is ", auc, ".\n", sep = "")
cat("Time for training xgboost model = ", tm_train_xgboost[1], " seconds \n", sep = "")
cat("Time for testing xgboost model = ", tm_test_xgboost[1], " seconds \n" , sep = "")
