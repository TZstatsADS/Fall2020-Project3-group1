# Chunk 1
packages.used <- c("R.matlab","readxl", "dplyr", "EBImage", "ggplot2", "caret", "glmnet", "WeightedROC","pROC","gbm","xgboost","randomForest","ranger", "magrittr", "e1071","grid","gridExtra")
# check packages that need to be installed.
packages.needed <- setdiff(packages.used, intersect(installed.packages()[,1], packages.used))
# install additional packages
if(length(packages.needed) > 0){
install.packages(packages.needed, dependencies = TRUE)
}
library(R.matlab)
library(readxl)
library(dplyr)
library(EBImage)
library(ggplot2)
library(caret)
library(glmnet)
library(WeightedROC)
library(pROC)
library(gbm)
library(xgboost)
library(randomForest)
library(ranger)
library(magrittr)
library(e1071)
library(grid)
library(gridExtra)
# Chunk 3: train_dir
train_dir <- "../data/train_set/" #may need to be changed to local directory
train_image_dir <- paste(train_dir, "images/", sep="")
train_pt_dir <- paste(train_dir,  "points/", sep="")
train_label_path <- paste(train_dir, "label.csv", sep="")
# Chunk 4: exp_setup
K <- 5  # number of CV folds
run.fudicial.list <- FALSE
run.feature.train <- FALSE # process features for training set
run.feature.test <- FALSE # process features for test set
sample.reweight <- TRUE # run sample reweighting in model training
run.cv.gbm <- FALSE # run cross-validation on the training set for gbm
run.train.gbm <- TRUE # run evaluation on entire train set
run.test.gbm <- TRUE # run evaluation on an independent test set
run.cv.xgboost <- FALSE # run cross-validation on the training set for xgboost
run.train.xgboost <- TRUE # run evaluation on entire train set
run.test.xgboost <- TRUE # run evaluation on an independent test set
run.cv.rforestw <- FALSE # run cross-validation on the training set for xgboost
run.train.rforestw <- FALSE # run evaluation on entire train set
run.test.rforestw <- TRUE # run evaluation on an independent test set
run.cv.RF <- FALSE # run cross-validation on the training set for xgboost
run.train.RF <- FALSE # run evaluation on entire train set
run.test.RF <- TRUE # run evaluation on an independent test set
run.cv.svm <- FALSE # run cross-validation on the training set for svm
run.train.svm <- FALSE # run evaluation on entire train set
run.test.svm <- TRUE # run evaluation on an independent test set
run.cv.pca <-FALSE # calculate pca
# Chunk 5: model_setup
# gbm model (baseline)
hyper_grid_gbm <- expand.grid(
shrinkage = c(0.001, 0.005, 0.010, 0.050, 0.100),
n.trees = c(600, 1200, 1800)
)
# xgboost model
hyper_grid_xgboost <- expand.grid(
eta = c(0.01, 0.05, 0.1, 0.2, 0.3),
lambda = c(0.001, 0.005, 0.010, 0.050, 0.100),
gamma = c(0, 5),
nrounds = c(600, 1200, 1800)
)
# svm model
hyper_grid_svm <- expand.grid(
nprinciple = c(400, 450, 500, 550, 600, 650, 700, 750)
)
# random forest model
hyper_grid_rf_uw <- expand.grid(
ntrees = c(100, 300, 500, 800, 1000),
mtry = c(500)
)
# random forest with weights model
hyper_grid_rforest <- expand.grid(
ntrees = c(100, 300, 500, 800, 1000),
maxd = c(0, 5, 10, 15, 20, 25)
)
# Chunk 6: train-test split
info <- read.csv(train_label_path)
n <- nrow(info)
n_train <- round(n*(4/5), 0)
train_idx <- sample(info$Index, n_train, replace = F)
test_idx <- setdiff(info$Index, train_idx)
# Chunk 7: read fiducial points
n_files <- length(list.files(train_image_dir))
if (run.fudicial.list){
readMat.matrix <- function(index){
return(round(readMat(paste0(train_pt_dir, sprintf("%04d", index), ".mat"))[[1]],0))
}
fiducial_pt_list <- lapply(1:n_files, readMat.matrix)
save(fiducial_pt_list, file="../output/fiducial_pt_list.RData")
# otherwise load the data stored for convenience
} else {
load(file="../output/fiducial_pt_list.RData")
}
# Chunk 8: feature
source("../lib/feature.R")
tm_feature_train <- NA
if(run.feature.train){
tm_feature_train <- system.time(dat_train <- feature(fiducial_pt_list, train_idx))
save(dat_train, tm_feature_train, file="../output/feature_train.RData")
}else{
load(file="../output/feature_train.RData")
}
tm_feature_test <- NA
if(run.feature.test){
tm_feature_test <- system.time(dat_test <- feature(fiducial_pt_list, test_idx))
save(dat_test, tm_feature_test, file="../output/feature_test.RData")
}else{
load(file="../output/feature_test.RData")
}
# Chunk 9: loadlib_gbm
source("../lib/train_gbm.R")
source("../lib/test_gbm.R")
source("../lib/cross_validation_gbm.R")
# Chunk 10: runcv_gbm
feature_train = as.matrix(dat_train[, -6007])
label_train = as.integer(dat_train$label)
if(run.cv.gbm){
res_cv <- matrix(0, nrow = nrow(hyper_grid_gbm), ncol = 4)
for(i in 1:nrow(hyper_grid_gbm)){
cat("n.trees = ", hyper_grid_gbm$n.trees[i], ",
shrinkage = ", hyper_grid_gbm$shrinkage[i],"\n", sep = "")
res_cv[i,] <- cv.function(features = feature_train, labels = label_train,
num_trees = hyper_grid_gbm$n.trees[i],
shrink = hyper_grid_gbm$shrinkage[i],
K, reweight = sample.reweight)
save(res_cv, file="../output/res_cv_gbm.RData")
}
}else{
load("../output/res_cv_gbm.RData")
}
# Chunk 11: optimized_parameter_gbm
res_cv_gbm <- as.data.frame(res_cv)
colnames(res_cv_gbm) <- c("mean_error", "sd_error", "mean_AUC", "sd_AUC")
gbm_cv_results = data.frame(hyper_grid_gbm, res_cv_gbm)
gbm_cv_results[order(gbm_cv_results$mean_AUC, decreasing = TRUE), ][1:5, ]
# Chunk 12: cv_vis_gbm_1
# Mean Error
p1 = ggplot(
gbm_cv_results, aes(as.factor(shrinkage), as.factor(n.trees), fill = mean_error)) +
geom_tile()+
labs(title="Mean Error Heatmap for gbm", y="ntrees", x="shrinkage")
# Mean AUC
p2 = ggplot(
gbm_cv_results, aes(as.factor(shrinkage), as.factor(n.trees), fill = mean_AUC)) +
geom_tile()+
labs(title="Mean AUC Heatmap for gbm",  y="ntrees", x="shrinkage")
grid.arrange(p1, p2,nrow=2)
# Chunk 13: cv_vis_gbm_2
# Mean Error
# N.Trees = 600
p1 <- ggplot(gbm_cv_results[gbm_cv_results$n.trees == 600, ],
aes(x = as.factor(shrinkage), y = mean_error,
ymin = mean_error - sd_error, ymax = mean_error + sd_error)) +
geom_crossbar() + theme(axis.text.x = element_text(angle = 90, hjust = 1))+
labs(title="Mean Error for gbm", subtitle="N.trees 600",
y="mean error", x="shrinkage")
# N.Trees = 1200
p2 = ggplot(gbm_cv_results[gbm_cv_results$n.trees == 1200, ],
aes(x = as.factor(shrinkage), y = mean_error,
ymin = mean_error - sd_error, ymax = mean_error + sd_error)) +
geom_crossbar() + theme(axis.text.x = element_text(angle = 90, hjust = 1))+
labs(title="Mean Error for gbm", subtitle="N.trees 1200",
y="mean error", x="shrinkage")
# N.Trees = 1800
p3 = ggplot(gbm_cv_results[gbm_cv_results$n.trees == 1800, ],
aes(x = as.factor(shrinkage), y = mean_error,
ymin = mean_error - sd_error, ymax = mean_error + sd_error)) +
geom_crossbar() + theme(axis.text.x = element_text(angle = 90, hjust = 1))+
labs(title="Mean Error for gbm", subtitle="N.trees 1800",
y="mean error", x="shrinkage")
# Mean AUC
# N.Trees = 600
p4 = ggplot(gbm_cv_results[gbm_cv_results$n.trees == 600, ],
aes(x = as.factor(shrinkage), y = mean_AUC,
ymin = mean_AUC - sd_AUC, ymax = mean_AUC + sd_AUC)) +
geom_crossbar() + theme(axis.text.x = element_text(angle = 90, hjust = 1))+
labs(title="Mean AUC for gbm", subtitle="N.trees 600",
y="mean AUC", x="shrinkage")
# N.Trees = 1200
p5 = ggplot(gbm_cv_results[gbm_cv_results$n.trees == 1200, ],
aes(x = as.factor(shrinkage), y = mean_AUC,
ymin = mean_AUC - sd_AUC, ymax = mean_AUC + sd_AUC)) +
geom_crossbar() + theme(axis.text.x = element_text(angle = 90, hjust = 1))+
labs(title="Mean AUC for gbm", subtitle="N.trees 1200",
y="mean AUC", x="shrinkage")
# N.Trees = 1800
p6 = ggplot(gbm_cv_results[gbm_cv_results$n.trees == 1800, ],
aes(x = as.factor(shrinkage), y = mean_AUC,
ymin = mean_AUC - sd_AUC, ymax = mean_AUC + sd_AUC)) +
geom_crossbar() + theme(axis.text.x = element_text(angle = 90, hjust = 1))+
labs(title="Mean AUC for gbm", subtitle="N.trees 1800",
y="mean AUC", x="shrinkage")
grid.arrange(p1, p2, p3,p4,p5,p6,ncol = 3,nrow=2)
# Chunk 14: best_model_gbm
gbm_cv_results[order(gbm_cv_results$mean_AUC, decreasing = TRUE), ][2, ]
par_best_gbm_ind <- 4
par_best_gbm_shrinkage <- gbm_cv_results$shrinkage[par_best_gbm_ind]
par_best_gbm_n.trees <- gbm_cv_results$n.trees[par_best_gbm_ind]
# Chunk 15: final_train_gbm
if (run.train.gbm) {
# training weights
weight_train <- rep(NA, length(label_train))
for (v in unique(label_train)){
weight_train[label_train == v] = 0.5 * length(label_train) /
length(label_train[label_train == v])
}
if (sample.reweight){
tm_train_gbm <- system.time(fit_train_gbm
<- train(feature_train, label_train, w = weight_train,
num_trees = par_best_gbm_n.trees,
shrink = par_best_gbm_shrinkage))
} else {
tm_train_gbm <- system.time(fit_train_gbm
<- train(feature_train, label_train, w = NULL,
num_trees = par_best_gbm_n.trees,
shrink = par_best_gbm_shrinkage))
}
save(fit_train_gbm, tm_train_gbm, file="../output/fit_train_gbm.RData")
} else {
load(file="../output/fit_train_gbm.RData")
}
# Chunk 16: test_gbm
tm_test_gbm = NA
feature_test <- as.matrix(dat_test[, -6007])
label_test <- as.integer(dat_test$label)
if(run.test.gbm){
load(file="../output/fit_train_gbm.RData")
tm_test_gbm <- system.time(
{prob_pred <- test(fit_train_gbm, feature_test, pred.type = 'response');
label_pred <- ifelse(prob_pred >= 0.5, 1, 0)})
}
# Chunk 17: evaluation_gbm
## reweight the test data to represent a balanced label distribution
weight_test <- rep(NA, length(label_test))
for (v in unique(label_test)){
weight_test[label_test == v] =
0.5 * length(label_test) / length(label_test[label_test == v])
}
# convert the original 1-2 class into numeric 0s and 1s
label_test <- ifelse(label_test == 2, 0, 1)
accu_gbm <- sum(weight_test * (label_pred == label_test)) / sum(weight_test)
tpr.fpr <- WeightedROC(prob_pred, label_test, weight_test)
auc_gbm<- WeightedAUC(tpr.fpr)
# Chunk 18: result_gbm
cat("The accuracy of the gbm model (",
"shinkage = ", par_best_gbm_shrinkage, ", n.trees = ",
par_best_gbm_n.trees, ") is ", accu_gbm*100, "%.\n", sep = "")
cat("The AUC of the gbm model (",
"shinkage = ", par_best_gbm_shrinkage, ", n.trees = ",
par_best_gbm_n.trees, ") is ", auc_gbm, ".\n", sep = "")
# Chunk 19: running_time_gbm
cat("Time for constructing training features = ", tm_feature_train[1], " seconds \n", sep = "")
cat("Time for constructing testing features = ", tm_feature_test[1], " seconds \n", sep = "")
cat("Time for training gbm model = ", tm_train_gbm[1], " seconds \n", sep = "")
cat("Time for testing gbm model = ", tm_test_gbm[1], " seconds \n" , sep = "")
# Chunk 20: loadlib_xgboost
source("../lib/train_xgboost.R")
source("../lib/test_xgboost.R")
source("../lib/cross_validation_xgboost.R")
# Chunk 21: runcv_xgboost
feature_train = as.matrix(dat_train[, -6007])
label_train = as.integer(dat_train$label)
if(run.cv.xgboost){
res_cv <- matrix(0, nrow = nrow(hyper_grid_xgboost), ncol = 4)
for (i in 1:nrow(hyper_grid_xgboost)){
print(i)
res_cv[i,] <- cv.function(features = feature_train, labels = label_train,
K,
eta_val = hyper_grid_xgboost$eta[i],
lmd = hyper_grid_xgboost$lambda[i],
gam = hyper_grid_xgboost$gamma[i],
nr = hyper_grid_xgboost$nrounds[i])
save(res_cv, file="../output/res_cv_xgboost.RData")
}
}else{
load("../output/res_cv_xgboost.RData")
}
# Chunk 22: optimized_parameter_xgboost
res_cv_xgboost <- as.data.frame(res_cv)
colnames(res_cv_xgboost) <- c("mean_error", "sd_error", "mean_AUC", "sd_AUC")
res_cv_xgboost_cv_results = data.frame(hyper_grid_xgboost, res_cv_xgboost)
res_cv_xgboost_cv_results[order(
res_cv_xgboost_cv_results$mean_AUC, decreasing = TRUE), ][1:5, ]
# Chunk 23: cv_vis_xgboost_1
# Mean Error
# gamma = 0, lambda = 0.05
p1 = ggplot(
res_cv_xgboost_cv_results[res_cv_xgboost_cv_results$gamma == 0 &
res_cv_xgboost_cv_results$lambda == 0.05, ],
aes(as.factor(nrounds), as.factor(eta), fill = mean_error)) +
geom_tile()+
labs(title="Mean Error Heatmap for xgboost",
subtitle="gamma 0 lambda 0.05", y="eta", x="nrounds")
# gamma = 0, lambda = 0.10
p2 = ggplot(
res_cv_xgboost_cv_results[res_cv_xgboost_cv_results$gamma == 0 &
res_cv_xgboost_cv_results$lambda == 0.10, ],
aes(as.factor(nrounds), as.factor(eta), fill = mean_error)) +
geom_tile()+
labs(title="Mean Error Heatmap for xgboost",
subtitle="gamma 0 lambda 0.10", y="eta", x="nrounds")
# Mean AUC
# gamma = 0, lambda = 0.05
p3 = ggplot(
res_cv_xgboost_cv_results[res_cv_xgboost_cv_results$gamma == 0 &
res_cv_xgboost_cv_results$lambda == 0.05, ],
aes(as.factor(nrounds), as.factor(eta), fill = mean_AUC)) +
geom_tile()+
labs(title="Mean AUC Heatmap for xgboost",
subtitle="gamma 0 lambda 0.05", y="eta", x="nrounds")
# gamma = 0, lambda = 0.10
p4 = ggplot(
res_cv_xgboost_cv_results[res_cv_xgboost_cv_results$gamma == 0 &
res_cv_xgboost_cv_results$lambda == 0.10, ],
aes(as.factor(nrounds), as.factor(eta), fill = mean_AUC)) +
geom_tile()+
labs(title="Mean AUC Heatmap for xgboost",
subtitle="gamma 0 lambda 0.10", y="eta", x="nrounds")
grid.arrange(p1, p2,p3,p4,ncol=2, nrow=2)
# Chunk 24: cv_vis_xgboost_2
# Mean Error
# nrounds = 600, gamma = 0, lambda = 0.05
p1 = ggplot(
res_cv_xgboost_cv_results[res_cv_xgboost_cv_results$nrounds == 600 &
res_cv_xgboost_cv_results$gamma == 0 &
res_cv_xgboost_cv_results$lambda == 0.05, ],
aes(x = as.factor(eta), y = mean_error,
ymin = mean_error - sd_error, ymax = mean_error + sd_error)) +
geom_crossbar() + theme(axis.text.x = element_text(angle = 90, hjust = 1))+
labs(title="Mean Error for xgboost", subtitle="nrounds 600, gamma 0, lambda 0.05",
y="mean error", x="eta")
# nrounds = 1200, gamma = 0, eta = 0.05
p2 = ggplot(
res_cv_xgboost_cv_results[res_cv_xgboost_cv_results$nrounds == 1200 &
res_cv_xgboost_cv_results$gamma == 0 &
res_cv_xgboost_cv_results$eta == 0.05, ],
aes(x = as.factor(lambda), y = mean_error,
ymin = mean_error - sd_error, ymax = mean_error + sd_error)) +
geom_crossbar() + theme(axis.text.x = element_text(angle = 90, hjust = 1))+
labs(title="Mean Error for xgboost", subtitle="nrounds 1200, gamma 0, eta 0.05",
y="mean error", x="lambda")
# Mean AUC
# nrounds = 600, gamma = 0, lambda = 0.05
p3 = ggplot(
res_cv_xgboost_cv_results[res_cv_xgboost_cv_results$nrounds == 600 &
res_cv_xgboost_cv_results$gamma == 0 &
res_cv_xgboost_cv_results$lambda == 0.05, ],
aes(x = as.factor(eta), y = mean_AUC,
ymin = mean_AUC - sd_AUC, ymax = mean_AUC + sd_AUC)) +
geom_crossbar() + theme(axis.text.x = element_text(angle = 90, hjust = 1))+
labs(title="Mean AUC for xgboost", subtitle="nrounds 600, gamma 0, lambda 0.05",
y="mean AUC", x="eta")
# nrounds = 1200, gamma = 0, eta = 0.05
p4 = ggplot(
res_cv_xgboost_cv_results[res_cv_xgboost_cv_results$nrounds == 1200 &
res_cv_xgboost_cv_results$gamma == 0 &
res_cv_xgboost_cv_results$eta == 0.05, ],
aes(x = as.factor(lambda), y = mean_AUC,
ymin = mean_AUC - sd_AUC, ymax = mean_AUC + sd_AUC)) +
geom_crossbar() + theme(axis.text.x = element_text(angle = 90, hjust = 1))+
labs(title="Mean AUC for xgboost", subtitle="nrounds 1200, gamma 0, eta 0.05",
y="mean AUC", x="lambda")
grid.arrange(p1, p2, p3,p4,ncol = 2,nrow=2)
# Chunk 25: best_model_xgboost
res_cv_xgboost_cv_results[
order(res_cv_xgboost_cv_results$mean_AUC, decreasing = TRUE), ][2, ]
par_best_res_cv_xgboost_cv_results_ind <- 23
par_best_res_cv_xgboost_cv_results_eta <-
res_cv_xgboost_cv_results$eta[par_best_res_cv_xgboost_cv_results_ind]
par_best_res_cv_xgboost_cv_results_lambda <-
res_cv_xgboost_cv_results$lambda[par_best_res_cv_xgboost_cv_results_ind]
par_best_res_cv_xgboost_cv_results_gamma <-
res_cv_xgboost_cv_results$gamma[par_best_res_cv_xgboost_cv_results_ind]
par_best_res_cv_xgboost_cv_results_nrounds <-
res_cv_xgboost_cv_results$nrounds[par_best_res_cv_xgboost_cv_results_ind]
# Chunk 26: final_train_xgboost
if (run.train.xgboost) {
weight_train <- rep(NA, length(label_train))
for (v in unique(label_train)){
weight_train[label_train == v] =
0.5 * length(label_train) / length(label_train[label_train == v])
}
if (sample.reweight){
tm_train_xgboost <- system.time(
fit_train_xgboost <- train(features = feature_train, labels = label_train,
w =  weight_train,
eta_val = par_best_res_cv_xgboost_cv_results_eta,
lmd = par_best_res_cv_xgboost_cv_results_lambda,
gam = par_best_res_cv_xgboost_cv_results_gamma,
nr = par_best_res_cv_xgboost_cv_results_nrounds))
}else{
tm_train_xgboost <- system.time(
fit_train_xgboost <- train(features = feature_train, labels = label_train,
w =  NULL,
eta_val = par_best_res_cv_xgboost_cv_results_eta,
lmd = par_best_res_cv_xgboost_cv_results_lambda,
gam = par_best_res_cv_xgboost_cv_results_gamma,
nr = par_best_res_cv_xgboost_cv_results_nrounds))
}
save(fit_train_xgboost, tm_train_xgboost, file="../output/fit_train_xgboost.RData")
}else {
load(file="../output/fit_train_xgboost.RData")
}
# Chunk 27: test_xgboost
tm_test_xgboost= NA
feature_test <- as.matrix(dat_test[, -6007])
label_test <- as.integer(dat_test$label)
if(run.test.xgboost){
load(file="../output/fit_train_xgboost.RData")
tm_test_xgboost <- system.time({prob_pred <- predict(fit_train_xgboost, feature_test);
label_pred <- ifelse(prob_pred >= 0.5, 1, 0)})
}
# Chunk 28: evaluation_xgboost
## reweight the test data to represent a balanced label distribution
weight_test <- rep(NA, length(label_test))
for (v in unique(label_test)){
weight_test[label_test == v] = 0.5 * length(label_test) /
length(label_test[label_test == v])
}
# convert the original 1-2 class into numeric 0s and 1s
label_test <- ifelse(label_test == 2, 0, 1)
accu_xgboost <- sum(weight_test * (label_pred == label_test)) / sum(weight_test)
tpr.fpr <- WeightedROC(prob_pred, label_test, weight_test)
auc_xgboost <- WeightedAUC(tpr.fpr)
# Chunk 29: result_xgboost
cat("The accuracy of the xgboost model (", "eta = ",
par_best_res_cv_xgboost_cv_results_eta,
", nrounds = ", par_best_res_cv_xgboost_cv_results_nrounds, ", lambda = ",
par_best_res_cv_xgboost_cv_results_lambda,
", gamma = ", par_best_res_cv_xgboost_cv_results_gamma,
") is ", accu_xgboost*100, "%.\n", sep = "")
cat("The AUC of the xgboost model (", "eta = ", par_best_res_cv_xgboost_cv_results_eta,
", nrounds = ", par_best_res_cv_xgboost_cv_results_nrounds, ", lambda = ",
par_best_res_cv_xgboost_cv_results_lambda,
", gamma = ", par_best_res_cv_xgboost_cv_results_gamma,
") is ", auc_xgboost, ".\n", sep = "")
# Chunk 30: running_time_xgboost
cat("Time for training xgboost model = ", tm_train_xgboost[1], " seconds \n", sep = "")
cat("Time for testing xgboost model = ", tm_test_xgboost[1], " seconds \n" , sep = "")
# Chunk 31: loadlib_svm
source("../lib/train_svm.R")
source("../lib/test_svm.R")
source("../lib/cross_validation_svm.R")
