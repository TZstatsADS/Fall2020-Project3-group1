---
title: "Main: Facial Expression Recognition Framework"
author: "Group 1: Kristen Akey, Levi Lee, Yiran Lin, Hanyi Yang, Wen Yin"
output:
  pdf_document: default
  html_notebook: default
editor_options: 
  chunk_output_type: console
---

# Introduction / Objective 

# Baseline Model/Proposed Model

# Results

# Analysis 


In your final repo, there should be an R markdown file that organizes **all computational steps** for evaluating your proposed Facial Expression Recognition framework. 

This file is currently a template for running evaluation experiments. You should update it according to your codes but following precisely the same structure. 

```{r message=FALSE, warning = FALSE, echo = FALSE, tidy=TRUE, tidy.opts=list(width.cutoff=60)}
if(!require("EBImage")){
  install.packages("BiocManager")
  BiocManager::install("EBImage")
}
if(!require("R.matlab")){
  install.packages("R.matlab")
}
if(!require("readxl")){
  install.packages("readxl")
}

if(!require("dplyr")){
  install.packages("dplyr")
}
if(!require("readxl")){
  install.packages("readxl")
}

if(!require("ggplot2")){
  install.packages("ggplot2")
}

if(!require("caret")){
  install.packages("caret")
}

if(!require("glmnet")){
  install.packages("glmnet")
}

if(!require("WeightedROC")){
  install.packages("WeightedROC")
}

if(!require("gbm")){
  install.packages("gbm")
}

if(!require("xgboost")){
  install.packages("xgboost")
}


if(!require("randomForest")){
  install.packages("randomForest")
}


library(R.matlab)
library(readxl)
library(dplyr)
library(EBImage)
library(ggplot2)
library(caret)
library(glmnet)
library(WeightedROC)

library(gbm)
library(xgboost)
library(randomForest)

# install other packages as needed using the format above 








```

### Step 0 set work directories
```{r wkdir, eval=FALSE}

set.seed(2020)
setwd("~/GitHub/Fall2020-Project3-group1/doc")


# change the working directory as needed
# if someone can make this a relative path, that would be great!!! 




```

Provide directories for training images. Training images and Training fiducial points will be in different subfolders.

```{r}
# change the directory of the data to where it's stored in your local drive as needed

# train_dir <- "../data/train_set/" # This will be modified for different data sets.

train_dir <- "~/train_set/"


train_image_dir <- paste(train_dir, "images/", sep="")
train_pt_dir <- paste(train_dir,  "points/", sep="")
train_label_path <- paste(train_dir, "label.csv", sep="") 

```

### Step 1: set up controls for evaluation experiments.

In this chunk, we have a set of controls for the evaluation experiments. 

+ (T/F) cross-validation on the training set
+ (T/F) reweighting the samples for training set 
+ (number) K, the number of CV folds
+ (T/F) process features for training set
+ (T/F) run evaluation on an independent test set
+ (T/F) process features for test set

```{r exp_setup}
K <- 5  # number of CV folds

run.fudicial.list <- FALSE
run.feature.train <- FALSE # process features for training set
run.feature.test <- FALSE # process features for test set
sample.reweight <- TRUE # run sample reweighting in model training

run.cv.gbm <- FALSE # run cross-validation on the training set for gbm 
run.train.gbm <- FALSE # run evaluation on entire train set
run.test.gbm <- TRUE # run evaluation on an independent test set

run.cv.xgboost <- FALSE # run cross-validation on the training set for xgboost 
run.train.xgboost <- FALSE # run evaluation on entire train set
run.test.xgboost <- TRUE # run evaluation on an independent test set

# add controls here to make if else statements to either cross-validate, test, train, or to just load saved data
# for xgboost, we need to also train and test each time we knit to record the time for the model 







```

Using cross-validation or independent test set evaluation, we compare the performance of models with different specifications. In this Starter Code, we tune parameter lambda (the amount of shrinkage) for logistic regression with LASSO penalty.

```{r model_setup}
# hyperparameters for our models 

# gbm model (baseline)
hyper_grid_gbm <- expand.grid(
  shrinkage = c(0.001, 0.005, 0.010, 0.050, 0.100),
  n.trees = c(600, 1200, 1800)
)

# xgboost model 
hyper_grid_xgboost <- expand.grid(
  eta = c(0.01, 0.05, 0.1, 0.2, 0.3),
  lambda = c(0.001, 0.005, 0.010, 0.050, 0.100),
  gamma = c(0, 5),
  nrounds = c(600, 1200, 1800)
)


# random forest model 










# add more hyperparameters for each model as needed 

```

### Step 2: import data and train-test split 
```{r}
#train-test split
info <- read.csv(train_label_path)
n <- nrow(info)
n_train <- round(n*(4/5), 0)
train_idx <- sample(info$Index, n_train, replace = F)
test_idx <- setdiff(info$Index, train_idx)
```


Fiducial points are stored in matlab format. In this step, we read them and store them in a list.
```{r read fiducial points}
n_files <- length(list.files(train_image_dir))

if (run.fudicial.list){
  #function to read fiducial points
  #input: index
  #output: matrix of fiducial points corresponding to the index
  readMat.matrix <- function(index){
       return(round(readMat(paste0(train_pt_dir, sprintf("%04d", index), ".mat"))[[1]],0))
  }
  
  #load fiducial points
  fiducial_pt_list <- lapply(1:n_files, readMat.matrix)
  save(fiducial_pt_list, file="../output/fiducial_pt_list.RData")
} else {
  load(file="../output/fiducial_pt_list.RData")
}
```

### Step 3: construct features and responses

+ The follow plots show how pairwise distance between fiducial points can work as feature for facial emotion recognition.

  + In the first column, 78 fiducials points of each emotion are marked in order. 
  + In the second column distributions of vertical distance between right pupil(1) and  right brow peak(21) are shown in  histograms. For example, the distance of an angry face tends to be shorter than that of a surprised face.
  + The third column is the distributions of vertical distances between right mouth corner(50)
and the midpoint of the upper lip(52).  For example, the distance of an happy face tends to be shorter than that of a sad face.

![Figure1](../figs/feature_visualization.jpg)

`feature.R` should be the wrapper for all your feature engineering functions and options. The function `feature( )` should have options that correspond to different scenarios for your project and produces an R object that contains features and responses that are required by all the models you are going to evaluate later. 
  
  + `feature.R`
  + Input: list of images or fiducial point
  + Output: an RData file that contains extracted features and corresponding responses

```{r feature}
source("../lib/feature.R")
tm_feature_train <- NA
if(run.feature.train){
  tm_feature_train <- system.time(dat_train <- feature(fiducial_pt_list, train_idx))
  save(dat_train, tm_feature_train, file="../output/feature_train.RData")
}else{
  load(file="../output/feature_train.RData")
}

tm_feature_test <- NA
if(run.feature.test){
  tm_feature_test <- system.time(dat_test <- feature(fiducial_pt_list, test_idx))
  save(dat_test, tm_feature_test, file="../output/feature_test.RData")
}else{
  load(file="../output/feature_test.RData")
}
```

\newpage

## Gradient Boosted Trees (gbm model) (Baseline Model)

### Step 4: Train a classification model with training features and responses
Call the train_gbm model and test_gbm model from library. 

`train_gbm.R` and `test_gbm.R` should be wrappers for all your model training steps and your classification/prediction steps. 

+ `train_gbm.R`
  + Input: a data frame containing features and labels and a parameter list.
  + Output:a trained model
+ `test_gbm.R`
  + Input: the fitted classification model using training data and processed features from testing images 
  + Input: an R object that contains a trained classifier.
  + Output: training model specification


```{r loadlib_gbm}
source("../lib/train_gbm.R") 
source("../lib/test_gbm.R")
source("../lib/cross_validation_gbm.R")
```

#### Model selection with cross-validation
* Do model selection by choosing among different values of training model parameters.

```{r runcv_gbm, message = FALSE}
feature_train = as.matrix(dat_train[, -6007])
label_train = as.integer(dat_train$label) 

if(run.cv.gbm){
  res_cv <- matrix(0, nrow = nrow(hyper_grid_gbm), ncol = 4)
  for(i in 1:nrow(hyper_grid_gbm)){
    cat("n.trees = ", hyper_grid_gbm$n.trees[i], ", 
        shrinkage = ", hyper_grid_gbm$shrinkage[i],"\n", sep = "")
    res_cv[i,] <- cv.function(features = feature_train, labels = label_train,
                              num_trees = hyper_grid_gbm$n.trees[i], 
                              shrink = hyper_grid_gbm$shrinkage[i], 
                              K, reweight = sample.reweight)
  save(res_cv, file="../output/res_cv_gbm.RData")
  }
}else{
  load("../output/res_cv_gbm.RData")
}
```

*Visualize cross-validation results. 
```{r cv_vis_gbm}
res_cv_gbm <- as.data.frame(res_cv) 
colnames(res_cv_gbm) <- c("mean_error", "sd_error", "mean_AUC", "sd_AUC")

gbm_cv_results = data.frame(hyper_grid_gbm, res_cv_gbm)

# a subset of cross-validated gbm models ordered by mean AUC (15 total)
# see appendix for full table
gbm_cv_results[order(gbm_cv_results$mean_AUC, decreasing = TRUE), ][1:5, ]

# Mean Error
ggplot(gbm_cv_results, aes(as.factor(shrinkage), as.factor(n.trees), fill = mean_error)) + 
  geom_tile()

# Mean AUC
ggplot(gbm_cv_results, aes(as.factor(shrinkage), as.factor(n.trees), fill = mean_AUC)) + 
  geom_tile()


# Mean Error
# N.Trees = 600
ggplot(gbm_cv_results[gbm_cv_results$n.trees == 600, ],
       aes(x = as.factor(shrinkage), y = mean_error, 
           ymin = mean_error - sd_error, ymax = mean_error + sd_error)) + 
    geom_crossbar() + theme(axis.text.x = element_text(angle = 90, hjust = 1))

# N.Trees = 1200
ggplot(gbm_cv_results[gbm_cv_results$n.trees == 1200, ],
       aes(x = as.factor(shrinkage), y = mean_error, 
           ymin = mean_error - sd_error, ymax = mean_error + sd_error)) + 
    geom_crossbar() + theme(axis.text.x = element_text(angle = 90, hjust = 1))

# N.Trees = 1800
ggplot(gbm_cv_results[gbm_cv_results$n.trees == 1800, ],
       aes(x = as.factor(shrinkage), y = mean_error, 
           ymin = mean_error - sd_error, ymax = mean_error + sd_error)) + 
    geom_crossbar() + theme(axis.text.x = element_text(angle = 90, hjust = 1))


# Mean AUC
# N.Trees = 600
ggplot(gbm_cv_results[gbm_cv_results$n.trees == 600, ],
       aes(x = as.factor(shrinkage), y = mean_AUC, 
           ymin = mean_AUC - sd_AUC, ymax = mean_AUC + sd_AUC)) + 
    geom_crossbar() + theme(axis.text.x = element_text(angle = 90, hjust = 1))

# N.Trees = 1200
ggplot(gbm_cv_results[gbm_cv_results$n.trees == 1200, ],
       aes(x = as.factor(shrinkage), y = mean_AUC, 
           ymin = mean_AUC - sd_AUC, ymax = mean_AUC + sd_AUC)) + 
    geom_crossbar() + theme(axis.text.x = element_text(angle = 90, hjust = 1))

# N.Trees = 1800
ggplot(gbm_cv_results[gbm_cv_results$n.trees == 1800, ],
       aes(x = as.factor(shrinkage), y = mean_AUC, 
           ymin = mean_AUC - sd_AUC, ymax = mean_AUC + sd_AUC)) + 
    geom_crossbar() + theme(axis.text.x = element_text(angle = 90, hjust = 1))
```


* Choose the "best" parameter value

Due to the presence imbalanced data, we choose to focus out attention on highest mean AUC rather than lowest mean error. However, we notice that the second best model (model 4) is has an mean AUC comparable to that of the best model (model 9) while being much simplier--model 4 has 600 trees while model 9 has 1200--we choose to select the more parsimonious model as our best baseline gbm model. 

```{r best_model_gbm}
gbm_cv_results[order(gbm_cv_results$mean_AUC, decreasing = TRUE), ][2, ]
par_best_gbm_ind <- 4
par_best_gbm_shrinkage <- gbm_cv_results$shrinkage[par_best_gbm_ind]
par_best_gbm_n.trees <- gbm_cv_results$n.trees[par_best_gbm_ind]
```

* Train the model with the entire training set using the selected model (model parameter) via cross-validation.

```{r final_train_gbm}
if (run.train.gbm) {
  # training weights
  weight_train <- rep(NA, length(label_train))
  for (v in unique(label_train)){
    weight_train[label_train == v] = 0.5 * length(label_train) / length(label_train[label_train == v])
  }
  
  if (sample.reweight){
    tm_train_gbm <- system.time(fit_train_gbm <- train(feature_train, label_train, w = weight_train, 
                                                       num_trees = par_best_gbm_n.trees, 
                                                       shrink = par_best_gbm_shrinkage))
  } else {
    tm_train_gbm <- system.time(fit_train_gbm <- train(feature_train, label_train, w = NULL, 
                                                       num_trees = par_best_gbm_n.trees,
                                                       shrink = par_best_gbm_shrinkage))
  }
  save(fit_train_gbm, tm_train_gbm, file="../output/fit_train_gbm.RData")
  
} else {
  load(file="../output/fit_train_gbm.RData")
}
```

### Step 5: Run test on test images

```{r test_gbm, message = FALSE}
feature_test <- as.matrix(dat_test[, -6007])
label_test <- as.integer(dat_test$label)

tm_test_gbm = NA

if(run.test.gbm){
  load(file="../output/fit_train_gbm.RData")
  tm_test_gbm <- system.time({prob_pred <- test(fit_train_gbm, feature_test, pred.type = 'response'); 
                              label_pred <- ifelse(prob_pred >= 0.5, 1, 0)})
}
```

* Evaluation
```{r, message = FALSE}
## reweight the test data to represent a balanced label distribution

weight_test <- rep(NA, length(label_test))
for (v in unique(label_test)){
  weight_test[label_test == v] = 0.5 * length(label_test) / length(label_test[label_test == v])
}


# convert the original 1-2 class into numeric 0s and 1s
label_test <- ifelse(label_test == 2, 0, 1)

accu <- sum(weight_test * (label_pred == label_test)) / sum(weight_test)
tpr.fpr <- WeightedROC(prob_pred, label_test, weight_test)
auc <- WeightedAUC(tpr.fpr)
```

```{r, echo = FALSE}
cat("The accuracy of the gbm model (", "shinkage = ", par_best_gbm_shrinkage, ", n.trees = ", par_best_gbm_n.trees, ") is ", accu*100, "%.\n", sep = "")
cat("The AUC of the gbm model (", "shinkage = ", par_best_gbm_shrinkage, ", n.trees = ", par_best_gbm_n.trees, ") is ", auc, ".\n", sep = "")
```


#### Summarize Running Time

Prediction performance matters, so does the running times for constructing features and for training the model, especially when the computation resource is limited. 


```{r running_time_gbm, echo = FALSE}
cat("Time for constructing training features = ", tm_feature_train[1], " seconds \n", sep = "")
cat("Time for constructing testing features = ", tm_feature_test[1], " seconds \n", sep = "")

cat("Time for training gbm model = ", tm_train_gbm[1], " seconds \n", sep = "") 
cat("Time for testing gbm model = ", tm_test_gbm[1], " seconds \n" , sep = "")
```


\newpage


## xgboost Model (Proposed Model) 

### Step 4: Train a classification model with training features and responses
Call the train model and test model from library. 

`train_xgboost.R` and `test_xgboost.R` should be wrappers for all your model training steps and your classification/prediction steps. 

+ `train_xgboost.R`
  + Input: a data frame containing features and labels and a parameter list.
  + Output:a trained model
+ `test_xgboost.R`
  + Input: the fitted classification model using training data and processed features from testing images 
  + Input: an R object that contains a trained classifier.
  + Output: training model specification


```{r loadlib_xgboost}
source("../lib/train_xgboost.R") 
source("../lib/test_xgboost.R") 
source("../lib/cross_validation_xgboost.R") 
```

#### Model selection with cross-validation
* Do model selection by choosing among different values of training model parameters.

```{r runcv_xgboost}
feature_train = as.matrix(dat_train[, -6007])
label_train = as.integer(dat_train$label) 

if(run.cv.xgboost){
  res_cv <- matrix(0, nrow = nrow(hyper_grid_xgboost), ncol = 4)
  for (i in 1:nrow(hyper_grid_xgboost)){
    print(i)
    res_cv[i,] <- cv.function(features = feature_train, labels = label_train,
                              K,
                              eta_val = hyper_grid_xgboost$eta[i], 
                              lmd = hyper_grid_xgboost$lambda[i], 
                              gam = hyper_grid_xgboost$gamma[i], 
                              nr = hyper_grid_xgboost$nrounds[i])
  save(res_cv, file="../output/res_cv_xgboost.RData")
  }
}else{
  load("../output/res_cv_xgboost.RData")
}
```

*Visualize cross-validation results. 
```{r cv_vis_xgboost}
res_cv_xgboost <- as.data.frame(res_cv) 
colnames(res_cv_xgboost) <- c("mean_error", "sd_error", "mean_AUC", "sd_AUC")

res_cv_xgboost_cv_results = data.frame(hyper_grid_xgboost, res_cv_xgboost)

# a subset of cross-validated xgboost models ordered by mean AUC (150 total)
# see appendix for full table
res_cv_xgboost_cv_results[order(res_cv_xgboost_cv_results$mean_AUC, decreasing = TRUE), ][1:5, ]

# Mean Error
# gamma = 0, lambda = 0.05
ggplot(res_cv_xgboost_cv_results[res_cv_xgboost_cv_results$gamma == 0 & res_cv_xgboost_cv_results$lambda == 0.05, ], aes(as.factor(nrounds), as.factor(eta), fill = mean_error)) + 
  geom_tile()
# gamma = 0, lambda = 0.10
ggplot(res_cv_xgboost_cv_results[res_cv_xgboost_cv_results$gamma == 0 & res_cv_xgboost_cv_results$lambda == 0.10, ], aes(as.factor(nrounds), as.factor(eta), fill = mean_error)) + 
  geom_tile()

# Mean AUC
# # gamma = 0, lambda = 0.05
ggplot(res_cv_xgboost_cv_results[res_cv_xgboost_cv_results$gamma == 0 & res_cv_xgboost_cv_results$lambda == 0.05, ], aes(as.factor(nrounds), as.factor(eta), fill = mean_AUC)) + 
  geom_tile()
# gamma = 0, lambda = 0.10
ggplot(res_cv_xgboost_cv_results[res_cv_xgboost_cv_results$gamma == 0 & res_cv_xgboost_cv_results$lambda == 0.10, ], aes(as.factor(nrounds), as.factor(eta), fill = mean_AUC)) + 
  geom_tile()


# Mean Error
# nrounds = 600
ggplot(res_cv_xgboost_cv_results[res_cv_xgboost_cv_results$nrounds == 600 & res_cv_xgboost_cv_results$gamma == 0 & res_cv_xgboost_cv_results$lambda == 0.05, ],
       aes(x = as.factor(eta), y = mean_error, 
           ymin = mean_error - sd_error, ymax = mean_error + sd_error)) + 
    geom_crossbar() + theme(axis.text.x = element_text(angle = 90, hjust = 1))

# nrounds = 1200
ggplot(res_cv_xgboost_cv_results[res_cv_xgboost_cv_results$nrounds == 1200 & res_cv_xgboost_cv_results$gamma == 0 & res_cv_xgboost_cv_results$lambda == 0.05, ],
       aes(x = as.factor(eta), y = mean_error, 
           ymin = mean_error - sd_error, ymax = mean_error + sd_error)) + 
    geom_crossbar() + theme(axis.text.x = element_text(angle = 90, hjust = 1))

# nrounds = 1800
ggplot(res_cv_xgboost_cv_results[res_cv_xgboost_cv_results$nrounds == 1800 & res_cv_xgboost_cv_results$gamma == 0 & res_cv_xgboost_cv_results$lambda == 0.05, ],
       aes(x = as.factor(eta), y = mean_error, 
           ymin = mean_error - sd_error, ymax = mean_error + sd_error)) + 
    geom_crossbar() + theme(axis.text.x = element_text(angle = 90, hjust = 1))


# Mean AUC
# nrounds = 600
ggplot(res_cv_xgboost_cv_results[res_cv_xgboost_cv_results$nrounds == 600 & res_cv_xgboost_cv_results$gamma == 0 & res_cv_xgboost_cv_results$lambda == 0.05, ],
       aes(x = as.factor(eta), y = mean_AUC, 
           ymin = mean_AUC - sd_AUC, ymax = mean_AUC + sd_AUC)) + 
    geom_crossbar() + theme(axis.text.x = element_text(angle = 90, hjust = 1))

# nrounds = 1200
ggplot(res_cv_xgboost_cv_results[res_cv_xgboost_cv_results$nrounds == 1200 & res_cv_xgboost_cv_results$gamma == 0 & res_cv_xgboost_cv_results$lambda == 0.05, ],
       aes(x = as.factor(eta), y = mean_AUC, 
           ymin = mean_AUC - sd_AUC, ymax = mean_AUC + sd_AUC)) + 
    geom_crossbar() + theme(axis.text.x = element_text(angle = 90, hjust = 1))

# nrounds = 1800
ggplot(res_cv_xgboost_cv_results[res_cv_xgboost_cv_results$nrounds == 1800 & res_cv_xgboost_cv_results$gamma == 0 & res_cv_xgboost_cv_results$lambda == 0.05, ],
       aes(x = as.factor(eta), y = mean_AUC, 
           ymin = mean_AUC - sd_AUC, ymax = mean_AUC + sd_AUC)) + 
    geom_crossbar() + theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

* Choose the "best" parameter value

Due to the presence imbalanced data, we choose to focus out attention on highest mean AUC rather than lowest mean error. However, we notice that the second best model (model 67) is has an mean AUC comparable to that of the best model (model 23) while being much simpler--model 23 has 600 trees while model 67 has 1200--we choose to select the more parsimonious model as our best proposed xgboost model. 

```{r best_model_xgboost}
res_cv_xgboost_cv_results[order(res_cv_xgboost_cv_results$mean_AUC, decreasing = TRUE), ][2, ]

par_best_res_cv_xgboost_cv_results_ind <- 23

par_best_res_cv_xgboost_cv_results_eta <- 
  res_cv_xgboost_cv_results$eta[par_best_res_cv_xgboost_cv_results_ind]
par_best_res_cv_xgboost_cv_results_lambda <- 
  res_cv_xgboost_cv_results$lambda[par_best_res_cv_xgboost_cv_results_ind]
par_best_res_cv_xgboost_cv_results_gamma <- 
  res_cv_xgboost_cv_results$gamma[par_best_res_cv_xgboost_cv_results_ind]
par_best_res_cv_xgboost_cv_results_nrounds <- 
  res_cv_xgboost_cv_results$nrounds[par_best_res_cv_xgboost_cv_results_ind]
```



* Train the model with the entire training set using the selected model (model parameter) via cross-validation.
```{r final_train_xgboost}
if (run.train.xgboost) {
  # training weights
  weight_train <- rep(NA, length(label_train))
  for (v in unique(label_train)){
    weight_train[label_train == v] = 0.5 * length(label_train) / length(label_train[label_train == v])
  }

  if (sample.reweight){
    tm_train_xgboost <- system.time(fit_train_xgboost <- train(features = feature_train, labels = label_train,
                                                                 w =  weight_train,
                                                                 eta_val = par_best_res_cv_xgboost_cv_results_eta,
                                                                 lmd = par_best_res_cv_xgboost_cv_results_lambda,
                                                                 gam = par_best_res_cv_xgboost_cv_results_gamma,
                                                                 nr = par_best_res_cv_xgboost_cv_results_nrounds))

  } else {

    tm_train_xgboost <- system.time(fit_train_xgboost <- train(features = feature_train, labels = label_train,
                                                                 w =  NULL, 
                                                                 eta_val = par_best_res_cv_xgboost_cv_results_eta, 
                                                                 lmd = par_best_res_cv_xgboost_cv_results_lambda,
                                                                 gam = par_best_res_cv_xgboost_cv_results_gamma,
                                                                 nr = par_best_res_cv_xgboost_cv_results_nrounds))
  }
  save(fit_train_xgboost, tm_train_xgboost, file="../output/fit_train_xgboost.RData")
  
} else {
  load(file="../output/fit_train_xgboost.RData")
}
```

### Step 5: Run test on test images

```{r}
feature_test <- as.matrix(dat_test[, -6007])
label_test <- as.integer(dat_test$label)

tm_test_xgboost= NA

if(run.test.xgboost){
  load(file="../output/fit_train_xgboost.RData")
  tm_test_xgboost <- system.time({prob_pred <- predict(fit_train_xgboost, feature_test); 
                              label_pred <- ifelse(prob_pred >= 0.5, 1, 0)})
}
```

* Evaluation

```{r}
## reweight the test data to represent a balanced label distribution
weight_test <- rep(NA, length(label_test))
for (v in unique(label_test)){
  weight_test[label_test == v] = 0.5 * length(label_test) / length(label_test[label_test == v])
}

# convert the original 1-2 class into numeric 0s and 1s
label_test <- ifelse(label_test == 2, 0, 1)

accu <- sum(weight_test * (label_pred == label_test)) / sum(weight_test)
tpr.fpr <- WeightedROC(prob_pred, label_test, weight_test)
auc <- WeightedAUC(tpr.fpr)
```


```{r, echo = FALSE}
cat("The accuracy of the xgboost model (", "eta = ", par_best_res_cv_xgboost_cv_results_eta, 
    ", nrounds = ", par_best_res_cv_xgboost_cv_results_nrounds, ", lambda = ", par_best_res_cv_xgboost_cv_results_lambda, 
    ", gamma = ", par_best_res_cv_xgboost_cv_results_gamma,
    ") is ", accu*100, "%.\n", sep = "")
cat("The AUC of the xgboost model (", "eta = ", par_best_res_cv_xgboost_cv_results_eta, 
    ", nrounds = ", par_best_res_cv_xgboost_cv_results_nrounds, ", lambda = ", par_best_res_cv_xgboost_cv_results_lambda, 
    ", gamma = ", par_best_res_cv_xgboost_cv_results_gamma,
    ") is ", auc, ".\n", sep = "")
```

#### Summarize Running Time

Prediction performance matters, so does the running times for constructing features and for training the model, especially when the computation resource is limited. 

```{r running_time_xgboost, echo = FALSE}
cat("Time for training xgboost model = ", tm_train_xgboost[1], " seconds \n", sep = "") 
cat("Time for testing xgboost model = ", tm_test_xgboost[1], " seconds \n" , sep = "")
```


\newpage

## Other Models 

## Principal Components Analysis (PCA) + Support Vector Machines (SVMs)

### Step 4: Train a classification model with training features and responses
Call the train model and test model from library. 

`train.R` and `test.R` should be wrappers for all your model training steps and your classification/prediction steps. 

+ `train.R`
  + Input: a data frame containing features and labels and a parameter list.
  + Output:a trained model
+ `test.R`
  + Input: the fitted classification model using training data and processed features from testing images 
  + Input: an R object that contains a trained classifier.
  + Output: training model specification

+ In this Starter Code, we use logistic regression with LASSO penalty to do classification. 

```{r loadlib_pca_svm}

```

#### Model selection with cross-validation
* Do model selection by choosing among different values of training model parameters.

```{r runcv_pca_svm}

```

*Visualize cross-validation results. 
```{r cv_vis_pcasvm}

```

* Choose the "best" parameter value

```{r best_model_pcasvm}

```

* Train the model with the entire training set using the selected model (model parameter) via cross-validation.

```{r final_train_pcasvm}

```

### Step 5: Run test on test images

```{r}

```

* Evaluation

```{r}

```


```{r, echo = FALSE}

```

#### Summarize Running Time

Prediction performance matters, so does the running times for constructing features and for training the model, especially when the computation resource is limited. 

```{r running_time_pcasvm, echo = FALSE}

```



\newpage

## Random Forests

### Step 4: Train a classification model with training features and responses
Call the train model and test model from library. 

`train.R` and `test.R` should be wrappers for all your model training steps and your classification/prediction steps. 

+ `train.R`
  + Input: a data frame containing features and labels and a parameter list.
  + Output:a trained model
+ `test.R`
  + Input: the fitted classification model using training data and processed features from testing images 
  + Input: an R object that contains a trained classifier.
  + Output: training model specification

+ In this Starter Code, we use logistic regression with LASSO penalty to do classification. 

```{r loadlib_rf, eval = FALSE}
library(magrittr)    #### NEED TO INSTALL THIS PACKAGE ON TO BUT ONLY IF WE STILL NEED IT
source("../lib/train.R") 
source("../lib/test.R")
```

#### Model selection with cross-validation
* Do model selection by choosing among different values of training model parameters.

```{r runcv_rf, eval = FALSE }
source("../lib/cross_validation.R")
lmbd = c(1e-3, 5e-3, 1e-2, 5e-2, 1e-1)
feature_train = as.matrix(dat_train[, -6007])
label_train = as.integer(dat_train$label) 
if(T){
  res_cv <- matrix(0, nrow = length(lmbd), ncol = 4)
  for(i in 1:length(lmbd)){
    cat("lambda = ", lmbd[i], "\n")
    res_cv[i,] <- cv.function(features = feature_train, labels = label_train, K, 
                              l = lmbd[i], reweight = sample.reweight)
  save(res_cv, file="../output/res_cv.RData")
  }
}else{
  load("../output/res_cv.RData")
```

*Visualize cross-validation results. 
```{r cv_vis_rf, eval = FALSE}
res_cv <- as.data.frame(res_cv) 
colnames(res_cv) <- c("mean_error", "sd_error", "mean_AUC", "sd_AUC")
res_cv$k = as.factor(lmbd)

if(run.cv){
  p1 <- res_cv %>% 
    ggplot(aes(x = as.factor(lmbd), y = mean_error,
               ymin = mean_error - sd_error, ymax = mean_error + sd_error)) + 
    geom_crossbar() +
    theme(axis.text.x = element_text(angle = 90, hjust = 1))
  
  p2 <- res_cv %>% 
    ggplot(aes(x = as.factor(lmbd), y = mean_AUC,
               ymin = mean_AUC - sd_AUC, ymax = mean_AUC + sd_AUC)) + 
    geom_crossbar() +
    theme(axis.text.x = element_text(angle = 90, hjust = 1))
  
  print(p1)
  print(p2)
}

```

* Choose the "best" parameter value

```{r best_model_rf, eval = FALSE}
par_best <- lmbd[which.min(res_cv$mean_error)] # lmbd[which.max(res_cv$mean_AUC)]
```

* Train the model with the entire training set using the selected model (model parameter) via cross-validation.

```{r final_train_rf, eval = FALSE}
# training weights
weight_train <- rep(NA, length(label_train))
for (v in unique(label_train)){
  weight_train[label_train == v] = 0.5 * length(label_train) / length(label_train[label_train == v])
}
if (sample.reweight){
  tm_train <- system.time(fit_train <- train(feature_train, label_train, w = weight_train, par_best))
} else {
  tm_train <- system.time(fit_train <- train(feature_train, label_train, w = NULL, par_best))
}
save(fit_train, file="../output/fit_train.RData")

```

### Step 5: Run test on test images

```{r, eval = FALSE}
tm_test = NA
feature_test <- as.matrix(dat_test[, -6007])
if(run.test){
  load(file="../output/fit_train.RData")
  tm_test <- system.time({label_pred <- as.integer(test(fit_train, feature_test, pred.type = 'class')); 
                          prob_pred <- test(fit_train, feature_test, pred.type = 'response')})
}

```

#### Evaluation

```{r, eval = FALSE}
## reweight the test data to represent a balanced label distribution
label_test <- as.integer(dat_test$label)
weight_test <- rep(NA, length(label_test))
for (v in unique(label_test)){
  weight_test[label_test == v] = 0.5 * length(label_test) / length(label_test[label_test == v])
}

accu <- sum(weight_test * (label_pred == label_test)) / sum(weight_test)
tpr.fpr <- WeightedROC(prob_pred, label_test, weight_test)
auc <- WeightedAUC(tpr.fpr)


cat("The accuracy of model:", model_labels[which.min(res_cv$mean_error)], "is", accu*100, "%.\n")
cat("The AUC of model:", model_labels[which.min(res_cv$mean_error)], "is", auc, ".\n")
```


```{r, echo = FALSE, eval = FALSE}
start=Sys.time()
rf_classifier = randomForest(label ~ ., data=dat_train, ntree=100, mtry=2000)
end=Sys.time()
rftraintime=end-start

start=Sys.time()
preds=predict(rf_classifier,dat_test[,-6007],type="class")
end=Sys.time()
rftesttime=end-start

#raw accuracy
rftestacc=sum(preds==dat_test$label)/length(dat_test$label)
predprob=predict(rf_classifier,dat_test[,-6007],type="prob")

#weighted accuracy
rfaccuw <- sum(weight_test * (preds == dat_test$label)) / sum(weight_test)
rftpr.fpr <- WeightedROC(predprob[,2], dat_test$label, weight_test)
rfauc <- WeightedAUC(rftpr.fpr)

cat("The unweighted accuracy of the random forest model is ", rftestacc*100, "%.\n")
cat("The weighted accuracy of the random forest model is ", rfaccuw*100, "%.\n")
cat("The AUC of model is", rfauc, ".\n")

```

#### Summarize Running Time

Prediction performance matters, so does the running times for constructing features and for training the model, especially when the computation resource is limited. 

```{r running_time_rf, echo = FALSE, eval = FALSE}
cat("Time for constructing training features=", tm_feature_train[1], "s \n")
cat("Time for constructing testing features=", tm_feature_test[1], "s \n")
cat("Time for training model=", tm_train[1], "s \n") 
cat("Time for testing model=", tm_test[1], "s \n")

cat("Time for training random forest model=", rftraintime, "min \n") 
cat("Time for testing random forest model=", rftesttime, "s \n")

```




\newpage

# Reference(s)
- Du, S., Tao, Y., & Martinez, A. M. (2014). Compound facial expressions of emotion. Proceedings of the National Academy of Sciences, 111(15), E1454-E1462.



# Appendix

We provide the full output of the cross-validation table for each model below. 

```{r}
gbm_cv_results

res_cv_xgboost_cv_results

```









